{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7f7ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 基础导入\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "# 项目模块导入\n",
    "from src.segmenter import fmm_segment, jieba_segment\n",
    "from src.pos_tagger import CRFPOSTagger\n",
    "from src.utils.data_loader import load_processed_data\n",
    "from src.train_bert import train_bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0daa8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_path = \"data/processed/train.json\"\n",
    "dev_path = \"data/processed/dev.json\"\n",
    "\n",
    "train_sents = load_processed_data(train_path)\n",
    "dev_sents = load_processed_data(dev_path)\n",
    "\n",
    "print(\"训练样本数:\", len(train_sents))\n",
    "print(\"开发集样本数:\", len(dev_sents))\n",
    "\n",
    "print(\"\\n示例样本：\")\n",
    "pprint(train_sents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4d9a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 统计句子长度\n",
    "train_lengths = [len(sent) for sent in train_sents]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(train_lengths, bins=30)\n",
    "plt.title(\"Train Sentence Length Distribution\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aae3ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tag_counter = Counter()\n",
    "\n",
    "for sent in train_sents:\n",
    "    for _, tag in sent:\n",
    "        tag_counter[tag] += 1\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(tag_counter.keys(), tag_counter.values())\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "print(\"标签统计：\")\n",
    "print(tag_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fd078",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"迈向充满希望的新世纪\"\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"FMM 分词结果:\", fmm_segment(sentence))\n",
    "print(\"Jieba 分词结果:\", jieba_segment(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc3db9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "crf = CRFPOSTagger()\n",
    "\n",
    "# 训练 CRF\n",
    "crf.train(train_sents)\n",
    "\n",
    "# 预测示例\n",
    "sample = [(\"迈\", \"B\"), (\"向\", \"E\")]\n",
    "print(\"CRF预测:\", crf.predict(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbe956",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "label_list = [\"B\", \"M\", \"E\", \"S\"]\n",
    "\n",
    "loss_list = train_bert(\n",
    "    train_path=train_path,\n",
    "    dev_path=dev_path,\n",
    "    label_list=label_list,\n",
    "    num_epochs=2\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_list)\n",
    "plt.title(\"BERT Training Loss Curve\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b7038",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "model_path = \"checkpoints/best_model\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "text = \"迈向充满希望的新世纪\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "logits = model(**tokens).logits\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0].tolist()\n",
    "\n",
    "print(\"Tokens:\", tokenizer.tokenize(text))\n",
    "print(\"Pred IDs:\", pred_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3b9a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def decode_to_words(text, pred_ids, label_list):\n",
    "    labels = [label_list[i] for i in pred_ids[1:len(text)+1]]  # 去掉 CLS\n",
    "    result = []\n",
    "    word = \"\"\n",
    "\n",
    "    for ch, tag in zip(text, labels):\n",
    "        if tag == \"B\":\n",
    "            if word:\n",
    "                result.append(word)\n",
    "            word = ch\n",
    "        elif tag == \"M\":\n",
    "            word += ch\n",
    "        elif tag == \"E\":\n",
    "            word += ch\n",
    "            result.append(word)\n",
    "            word = \"\"\n",
    "        elif tag == \"S\":\n",
    "            result.append(ch)\n",
    "\n",
    "    if word:\n",
    "        result.append(word)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"预测分词：\", decode_to_words(text, pred_ids, label_list))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
